{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **#TelegramScrap: A comprehensive tool for scraping Telegram data**\n",
        "\n",
        "✅ This code, developed by [Ergon Cugler de Moraes Silva](https://github.com/ergoncugler) (Brazil), aims to scrape data from selected `Telegram Channels, Groups, or Chats` using the `Telethon Library`. It is designed to facilitate the extraction of various data fields including `message content, author information, reactions, views, and comments`. The primary functions of this code include setting up scraping parameters, processing messages and their associated comments, and handling unsupported characters to ensure data integrity. Data is stored in `Apache Parquet files (.parquet)`, which are highly efficient for both storage and processing, making them superior to traditional spreadsheets in terms of speed and scalability. This tool is particularly **useful for researchers and analysts** looking to collect and analyze Telegram data efficiently.\n",
        "\n",
        "✅ **The code is open-source and available for free at [https://github.com/ergoncugler/web-scraping-telegram/](https://github.com/ergoncugler/web-scraping-telegram/)**. While it is free to use and modify, the responsibility for its use and any modifications lies with the user. Feel free to explore, utilize, and adapt the code to suit your needs, but please ensure you comply with Telegram's terms of service and data privacy regulations.\n",
        "\n",
        "## **#Papers: Some scientific production using this code**\n",
        "\n",
        "✅ In the realm of scientific articles, this code was instrumental in the study **Informational Co-option against Democracy: Comparing Bolsonaro's Discourses about Voting Machines with the Public Debate** ([**link**](https://dl.acm.org/doi/abs/10.1145/3614321.3614373)). It was also used in **Institutional Denialism From the President's Speeches to the Formation of the Early Treatment Agenda (Off Label) in the COVID-19 Pandemic in Brazil** ([**link**](https://anepecp.org/ojs/index.php/br/article/view/561)). Moreover, the code facilitated research in **Catalytic Conspiracism: Exploring Persistent Homologies Time Series in the Dissemination of Disinformation in Conspiracy Theory Communities on Telegram** ([**link**](https://www.abcp2024.sinteseeventos.com.br/trabalho/view?ID_TRABALHO=687)) and **Conspiratorial Convergence: Comparing Thematic Agendas Among Conspiracy Theory Communities on Telegram Using Topic Modeling** ([**link**](https://www.abcp2024.sinteseeventos.com.br/trabalho/view?ID_TRABALHO=903)). Lastly, it was pivotal in the study **Informational Disorder and Institutions Under Attack: How Did Former President Bolsonaro's Narratives Against the Brazilian Judiciary Between 2019 and 2022 Manifest?** ([**link**](https://www.encontro2023.anpocs.org.br/trabalho/view?ID_TRABALHO=8990)).\n",
        "\n",
        "✅ Furthermore, the code was utilized in several technical notes, as we can see in **Technical Note #16 – Disinformation about Electronic Voting Machines Persists Outside Election Periods** ([**link**](https://www.monitordigital.org/2023/05/18/nota-tecnica-16-desinformacao-sobre-urnas-eletronicas-persiste-fora-dos-periodos-eleitorais/)). It was also employed in **Technical Note #18 – Electoral Fraud Discourse in Argentina on Telegram and Twitter** ([**link**](https://www.monitordigital.org/2023/10/24/nota-tecnica-18-discurso-de-fraude-eleitoral-na-argentina-no-telegram-e-no-twitter/)). The code contributed to the analysis in the technical note **Bashing and Praising Public Servants and Bureaucrats During the Bolsonaro Government (2019 - 2022)** ([**link**](https://neburocracia.wordpress.com/wp-content/uploads/2024/04/nota-tecnica-neb-fgv-eaesp-como-bolsonaro-equilibrou-ataques-e-acenos-aos-servidores-publicos-e-burocratas-entre-2019-e-2022.pdf)). Additionally, it was used in **Technical Note 2: The Digital Territory of Milei's Followers: From Commerce to Politics** ([**link**](https://pacunla.com/nota-tecnica-2-el-territorio-digital-de-los-seguidores-de-milei-del-comercio-a-la-politica/)).\n",
        "\n",
        "✅ To credit this academic work and the scraping code, it is recommended to cite: **SILVA, Ergon Cugler de Moraes. *Web Scraping Telegram Posts and Content*. (Feb) 2023. Available at: [https://github.com/ergoncugler/web-scraping-telegram/](https://github.com/ergoncugler/web-scraping-telegram/).**"
      ],
      "metadata": {
        "id": "9eNktQvRmQV0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg_TE53e4FS8"
      },
      "source": [
        "## **Setup**\n",
        "\n",
        "Here you need to input your credentials like `username, phone, api_id and api_hash`. Your api_id and your api_hash, **it can be only generated from https://my.telegram.org/apps**. Once you set your details for the first time, you no longer need to update, just click play."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0rBWHyjStKt"
      },
      "outputs": [],
      "source": [
        "# Install the Telethon library for Telegram API interactions\n",
        "!pip install telethon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEn8Clkqe0dh"
      },
      "outputs": [],
      "source": [
        "# Initial imports\n",
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Telegram imports\n",
        "from telethon.sync import TelegramClient\n",
        "\n",
        "# Google Colab imports\n",
        "from google.colab import files\n",
        "\n",
        "# Setup / change only the first time you use it\n",
        "username = 'your_username'  # Your Telegram account username (just 'abc123', not '@')\n",
        "phone = '+5511999999999'  # Your Telegram account phone number (ex: '+5511999999999')\n",
        "api_id = '11111111'  # Your API ID, it can be only generated from https://my.telegram.org/apps\n",
        "api_hash = '1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a'  # Your API hash, also from https://my.telegram.org/apps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgRaIqPy4JFS"
      },
      "source": [
        "## **Scraping**\n",
        "\n",
        "In this section, you will define the parameters for scraping data from Telegram channels or groups. Specify the channels you want to scrape using the format `@ChannelName` or the full URL `https://t.me/ChannelName`. Do not use URLs starting with `https://web.telegram.org/`. Set the date range by defining the start and end day, month, and year. Choose an output file name for the scraped data. Optionally, set a search keyword if you need to filter messages by specific terms. Define the maximum number of messages to scrape and set a timeout in seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbj-yuEZ9ioh"
      },
      "outputs": [],
      "source": [
        "# Setup / change every time to define scraping parameters\n",
        "channels = [\n",
        "    '@LulanoTelegram',\n",
        "    '@jairbolsonarobrasil',\n",
        "]\n",
        "# Here you put the name of the channel or group that you want to scrape\n",
        "# As an example, play: '@LulanoTelegram' or 'https://t.me/LulanoTelegram'\n",
        "# Do not use: 'https://web.telegram.org/a/#-1001249230829' or '-1001249230829'\n",
        "\n",
        "d_min = 1  # Start day (inclusive)\n",
        "m_min = 1  # Start month\n",
        "y_min = 2000  # Start year\n",
        "d_max = 1  # End day (exclusive)\n",
        "m_max = 8  # End month\n",
        "y_max = 2024  # End year\n",
        "file_name = 'Test'  # Output file name\n",
        "key_search = ''  # Keyword to search, leave empty if not needed\n",
        "max_t_index = 1000000  # Maximum number of messages to scrape\n",
        "time_limit = 6 * 60 * 60  # Timeout in hours (*seconds)\n",
        "File = 'parquet'  # Set to 'parquet' or 'excel'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCj3aYBhzSiX"
      },
      "outputs": [],
      "source": [
        "data = []  # List to store scraped data\n",
        "t_index = 0  # Tracker for the number of messages processed\n",
        "start_time = time.time()  # Record the start time for the scraping session\n",
        "\n",
        "# Function to remove invalid XML characters from text\n",
        "def remove_unsupported_characters(text):\n",
        "    valid_xml_chars = (\n",
        "        \"[^\\u0009\\u000A\\u000D\\u0020-\\uD7FF\\uE000-\\uFFFD\"\n",
        "        \"\\U00010000-\\U0010FFFF]\"\n",
        "    )\n",
        "    cleaned_text = re.sub(valid_xml_chars, '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to format time in days, hours, minutes, and seconds\n",
        "def format_time(seconds):\n",
        "    days = seconds // 86400\n",
        "    hours = (seconds % 86400) // 3600\n",
        "    minutes = (seconds % 3600) // 60\n",
        "    seconds = seconds % 60\n",
        "    return f'{int(days):02}:{int(hours):02}:{int(minutes):02}:{int(seconds):02}'\n",
        "\n",
        "# Function to print progress of the scraping process\n",
        "def print_progress(t_index, message_id, start_time, max_t_index):\n",
        "    elapsed_time = time.time() - start_time\n",
        "    current_progress = t_index / (t_index + message_id) if (t_index + message_id) <= max_t_index else t_index / max_t_index\n",
        "    percentage = current_progress * 100\n",
        "    estimated_total_time = elapsed_time / current_progress\n",
        "    remaining_time = estimated_total_time - elapsed_time\n",
        "\n",
        "    elapsed_time_str = format_time(elapsed_time)\n",
        "    remaining_time_str = format_time(remaining_time)\n",
        "\n",
        "    print(f'Progress: {percentage:.2f}% | Elapsed Time: {elapsed_time_str} | Remaining Time: {remaining_time_str}')\n",
        "\n",
        "# Scraping process\n",
        "for channel in channels:\n",
        "    if t_index >= max_t_index:\n",
        "        break\n",
        "\n",
        "    if time.time() - start_time > time_limit:\n",
        "        break\n",
        "\n",
        "    loop_start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        c_index = 0\n",
        "        async with TelegramClient(username, api_id, api_hash) as client:\n",
        "            async for message in client.iter_messages(channel, search=key_search):\n",
        "                try:\n",
        "                    if datetime(y_min, m_min, d_min, tzinfo=timezone.utc) < message.date <= datetime(y_max, m_max, d_max, tzinfo=timezone.utc):\n",
        "\n",
        "                        # Process comments of the message\n",
        "                        comments_list = []\n",
        "                        try:\n",
        "                            async for comment_message in client.iter_messages(channel, reply_to=message.id):\n",
        "                                comment_text = comment_message.text.replace(\"'\", '\"')\n",
        "\n",
        "                                comment_media = 'True' if comment_message.media else 'False'\n",
        "\n",
        "                                comment_emoji_string = ''\n",
        "                                if comment_message.reactions:\n",
        "                                    for reaction_count in comment_message.reactions.results:\n",
        "                                        emoji = reaction_count.reaction.emoticon\n",
        "                                        count = str(reaction_count.count)\n",
        "                                        comment_emoji_string += emoji + \" \" + count + \" \"\n",
        "\n",
        "                                comment_date_time = comment_message.date.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "                                comments_list.append({\n",
        "                                    'Type': 'comment',\n",
        "                                    'Comment Group': channel,\n",
        "                                    'Comment Author ID': comment_message.sender_id,\n",
        "                                    'Comment Content': comment_text,\n",
        "                                    'Comment Date': comment_date_time,\n",
        "                                    'Comment Message ID': comment_message.id,\n",
        "                                    'Comment Author': comment_message.post_author,\n",
        "                                    'Comment Views': comment_message.views,\n",
        "                                    'Comment Reactions': comment_emoji_string,\n",
        "                                    'Comment Shares': comment_message.forwards,\n",
        "                                    'Comment Media': comment_media,\n",
        "                                    'Comment Url': f'https://t.me/{channel}/{message.id}?comment={comment_message.id}'.replace('@', ''),\n",
        "                                })\n",
        "                        except Exception as e:\n",
        "                            comments_list = []\n",
        "                            print(f'Error processing comments: {e}')\n",
        "\n",
        "                        # Process the main message\n",
        "                        media = 'True' if message.media else 'False'\n",
        "\n",
        "                        emoji_string = ''\n",
        "                        if message.reactions:\n",
        "                            for reaction_count in message.reactions.results:\n",
        "                                emoji = reaction_count.reaction.emoticon\n",
        "                                count = str(reaction_count.count)\n",
        "                                emoji_string += emoji + \" \" + count + \" \"\n",
        "\n",
        "                        date_time = message.date.strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        cleaned_content = remove_unsupported_characters(message.text)\n",
        "                        cleaned_comments_list = remove_unsupported_characters(json.dumps(comments_list))\n",
        "\n",
        "                        data.append({\n",
        "                            'Type': 'text',\n",
        "                            'Group': channel,\n",
        "                            'Author ID': message.sender_id,\n",
        "                            'Content': cleaned_content,\n",
        "                            'Date': date_time,\n",
        "                            'Message ID': message.id,\n",
        "                            'Author': message.post_author,\n",
        "                            'Views': message.views,\n",
        "                            'Reactions': emoji_string,\n",
        "                            'Shares': message.forwards,\n",
        "                            'Media': media,\n",
        "                            'Url': f'https://t.me/{channel}/{message.id}'.replace('@', ''),\n",
        "                            'Comments List': cleaned_comments_list,\n",
        "                        })\n",
        "\n",
        "                        c_index += 1\n",
        "                        t_index += 1\n",
        "\n",
        "                        # Print progress\n",
        "                        print(f'{\"-\" * 80}')\n",
        "                        print_progress(t_index, message.id, start_time, max_t_index)\n",
        "                        current_max_id = min(c_index + message.id, max_t_index)\n",
        "                        print(f'From {channel}: {c_index:05} contents of {current_max_id:05}')\n",
        "                        print(f'Id: {message.id:05} / Date: {date_time}')\n",
        "                        print(f'Total: {t_index:05} contents until now')\n",
        "                        print(f'{\"-\" * 80}\\n\\n')\n",
        "\n",
        "                        if t_index % 1000 == 0:\n",
        "                            if File == 'parquet':\n",
        "                                backup_filename = f'backup_{file_name}_until_{t_index:05}_{channel}_ID{message.id:07}.parquet'\n",
        "                                pd.DataFrame(data).to_parquet(backup_filename, index=False)\n",
        "                            elif File == 'excel':\n",
        "                                backup_filename = f'backup_{file_name}_until_{t_index:05}_{channel}_ID{message.id:07}.xlsx'\n",
        "                                pd.DataFrame(data).to_excel(backup_filename, index=False)\n",
        "\n",
        "                        if t_index >= max_t_index:\n",
        "                            break\n",
        "\n",
        "                        if time.time() - start_time > time_limit:\n",
        "                            break\n",
        "\n",
        "                    elif message.date < datetime(y_min, m_min, d_min, tzinfo=timezone.utc):\n",
        "                        break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f'Error processing message: {e}')\n",
        "\n",
        "        print(f'\\n\\n##### {channel} was ok with {c_index:05} posts #####\\n\\n')\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        if File == 'parquet':\n",
        "            partial_filename = f'complete_{channel}_in_{file_name}_until_{t_index:05}.parquet'\n",
        "            df.to_parquet(partial_filename, index=False)\n",
        "        elif File == 'excel':\n",
        "            partial_filename = f'complete_{channel}_in_{file_name}_until_{t_index:05}.xlsx'\n",
        "            df.to_excel(partial_filename, index=False)\n",
        "        # files.download(partial_filename)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'{channel} error: {e}')\n",
        "\n",
        "    loop_end_time = time.time()\n",
        "    loop_duration = loop_end_time - loop_start_time\n",
        "\n",
        "    if loop_duration < 60:\n",
        "        time.sleep(60 - loop_duration)\n",
        "\n",
        "print(f'\\n{\"-\" * 50}\\n#Concluded! #{t_index:05} posts were scraped!\\n{\"-\" * 50}\\n\\n\\n\\n')\n",
        "df = pd.DataFrame(data)\n",
        "if File == 'parquet':\n",
        "    final_filename = f'FINAL_{file_name}_with_{t_index:05}.parquet'\n",
        "    df.to_parquet(final_filename, index=False)\n",
        "elif File == 'excel':\n",
        "    final_filename = f'FINAL_{file_name}_with_{t_index:05}.xlsx'\n",
        "    df.to_excel(final_filename, index=False)\n",
        "files.download(final_filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT9a0E2hVrg-"
      },
      "source": [
        "## **Reading**\n",
        "\n",
        "If you want to read your generated file or convert from `.parquet` to another format, feel free to use it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK8dL6HFVrg_"
      },
      "outputs": [],
      "source": [
        "# Import pandas for data manipulation\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "filename = 'TYPE_HERE_YOUR_FILENAME.parquet'  # Set the Parquet file name here, including the .parquet extension\n",
        "\n",
        "# Read the final Parquet file\n",
        "df_parquet = pd.read_parquet(filename)\n",
        "\n",
        "# Display the dataframe to the user (optional)\n",
        "display(df_parquet)\n",
        "\n",
        "# Convert the dataframe to Excel and set the new filename\n",
        "excel_filename = filename.replace('.parquet', '.xlsx')\n",
        "\n",
        "# Save the dataframe as an Excel file\n",
        "df_parquet.to_excel(excel_filename, index=False)\n",
        "\n",
        "# Download the Excel file\n",
        "files.download(excel_filename)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Wg_TE53e4FS8",
        "hT9a0E2hVrg-"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
